{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T05:47:27.349896Z",
     "start_time": "2023-07-03T05:47:25.934247Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from yahoo_fin import options as op\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_sigma:\n",
    "\n",
    "    def __init__(self, ticker) -> None:\n",
    "\n",
    "        yfin = yf.Ticker(ticker)\n",
    "        da = yfin.history(period = '3mo')\n",
    "        self.price_list =  list(da.iloc[:,3])\n",
    "        self.return_list = []\n",
    "        self.price = self.price_list[-1]\n",
    "\n",
    "    def calculation(self):\n",
    "\n",
    "        for i in range(1, len(self.price_list)):\n",
    "            a = np.log(((self.price_list[i]-self.price_list[i-1])/self.price_list[i-1] + 1))\n",
    "            self.return_list.append(a)\n",
    "        self.return_array = self.return_list\n",
    "        self.mu = np.mean(self.return_array)\n",
    "        self.sigma = np.sqrt((1 / (len(self.return_list) - 1)) * np.sum(((self.return_array - self.mu) ** 2)))\n",
    "        self.mu_year = self.mu * 252\n",
    "        self.sigma_year = self.sigma * np.sqrt(252)\n",
    "\n",
    "\n",
    "class BlackScholes:\n",
    "\n",
    "    def __init__(self, s, e, sigma, r, delta_t, dividend=0):\n",
    "\n",
    "        self.S = s\n",
    "        self.E = e\n",
    "        self.sigma = sigma\n",
    "        self.r = r\n",
    "        self.delta_t = delta_t\n",
    "        self.dividend = dividend\n",
    "        partial = self.r - self.dividend + (1/2)*self.sigma**2\n",
    "        self.d1 = (np.log(self.S/self.E)+partial*self.delta_t)/(self.sigma*np.sqrt(self.delta_t))\n",
    "        self.d2 = self.d1 - self.sigma*np.sqrt(self.delta_t)\n",
    "\n",
    "    def black_scholes(self):\n",
    "\n",
    "        Part_E = self.E * np.exp(-self.r * self.delta_t) * stats.norm.cdf(self.d2)\n",
    "        Part_S = self.S * np.exp(-self.dividend * self.delta_t) * stats.norm.cdf(self.d1)\n",
    "        self.Call = Part_S - Part_E\n",
    "        self.Put = self.Call - self.S + self.E * np.exp(-self.r * self.delta_t)\n",
    "\n",
    "\n",
    "\n",
    "class JumpDiffusion:\n",
    "\n",
    "    \"\"\"                 dS(t)\n",
    "                        ----- = mu*dt + sigma*dW(t) + dJ(t)\n",
    "                        S(t-)\n",
    "\n",
    "                        S = Current Stock Price\n",
    "                        E = Strike\n",
    "                        T = Time to maturity in years\n",
    "                        σ = Annual Volatility\n",
    "                        m = Mean of Jump Size\n",
    "                        v = Standard Deviation of Jump Size\n",
    "                        λ = Number of jumps per year (intensity)\n",
    "                        dW(t) = Weiner Process\n",
    "                        N(t) = Compound Poisson Process\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s, e, sigma, r, T, m, v, lamb):\n",
    "\n",
    "        self.s = s\n",
    "        self.e = e\n",
    "        self.sigma = sigma\n",
    "        self.r = r\n",
    "        self.T = T\n",
    "        self.m = m\n",
    "        self.v = v\n",
    "        self.lam = lamb\n",
    "\n",
    "    def JD_Call(self):\n",
    "        call_price = 0\n",
    "        for k in range(50):\n",
    "            r_k = self.r - self.lam * (self.m - 1) + (k * np.log(self.m) ) / self.T\n",
    "            sigma_k = np.sqrt(self.sigma ** 2 + (k * self.v ** 2) / self.T)\n",
    "            k_fact = np.math.factorial(k)\n",
    "            bs = BlackScholes(self.s, self.e, sigma_k, r_k, self.T)\n",
    "            bs.black_scholes()\n",
    "            self.bs_call = bs.Call\n",
    "            call_price += (np.exp(- self.m * self.lam * self.T) * (self.m * self.lam * self.T) ** k / (k_fact)) * self.bs_call\n",
    "        return call_price\n",
    "\n",
    "    def JD_Put(self):\n",
    "        put_price = 0\n",
    "        for k in range(50):\n",
    "            r_k = self.r - self.lam * (self.m - 1) + (k * np.log(self.m) ) / self.T\n",
    "            sigma_k = np.sqrt(self.sigma ** 2 + (k * self.v ** 2) / self.T)\n",
    "            k_fact = np.math.factorial(k)\n",
    "            bs = BlackScholes(self.s, self.e, sigma_k, r_k, self.T)\n",
    "            bs.black_scholes()\n",
    "            self.bs_put = bs.Put\n",
    "            put_price += (np.exp(- self.m * self.lam * self.T) * (self.m * self.lam * self.T) ** k / (k_fact)) * self.bs_put\n",
    "        return put_price\n",
    "\n",
    "\n",
    "def get_ticker():\n",
    "\n",
    "    sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "    return list(sp500[0]['Symbol'])\n",
    "\n",
    "def get_stock(ticker):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(len(ticker)):\n",
    "        yfin = yf.Ticker(ticker[i])\n",
    "        da = yfin.history(period = '1mo').iloc[-1:,[0,3]]\n",
    "        da['ticker'] = ticker[i]\n",
    "        data = pd.concat([data, da])\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_call(ticker):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    ed = op.get_expiration_dates(ticker)\n",
    "\n",
    "    for i in range(len(ed)):\n",
    "\n",
    "        call_data = op.get_calls(ticker,ed[i])\n",
    "        time = (parser.parse(ed[i]).date() - datetime.now().date())\n",
    "        call_data['Time'] = ed[i]\n",
    "        call_data['Expiration'] = time.days / 365\n",
    "        data = pd.concat([data, call_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_call_by_ticker(ticker):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(len(ticker)):\n",
    "        data = pd.concat([data, get_call(ticker[i])])\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_full_data(ticker):\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    return yf.download(ticker,'2015-01-01',today)[['Open','Close']]\n",
    "\n",
    "def get_daily_change(tickers):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        df = yf.download(ticker, start='2021-01-01', end=datetime.today().strftime('%Y-%m-%d'))\n",
    "        df['Daily Change'] = df['Close'].pct_change()\n",
    "        df = df[['Daily Change']].dropna()\n",
    "        df.rename(columns={'Daily Change': ticker}, inplace=True)\n",
    "        data = pd.concat([data, df], axis=1)\n",
    "\n",
    "    data = data.transpose()\n",
    "    data.columns = ['day' + str(i + 1) for i in range(len(data.columns))]\n",
    "    data.insert(0, 'stockname', data.index)\n",
    "\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "class work:\n",
    "\n",
    "    def __init__(self, ticker, time_number = 3, Strike_number = 2):\n",
    "\n",
    "        self.ticker_stock = get_stock([ticker])\n",
    "        ticker_option = get_call(ticker)\n",
    "        self.time = ticker_option['Expiration'].unique()[time_number]\n",
    "        self.option_time = ticker_option[ticker_option['Expiration'] == self.time]\n",
    "        sigma = get_sigma(ticker)\n",
    "        sigma.calculation()\n",
    "        self.sigma = sigma.sigma_year\n",
    "        self.Strike = Strike_number\n",
    "\n",
    "    def stupid_simu(self):\n",
    "\n",
    "        self.call_jd = pd.DataFrame(columns=[\"lambda\", \"mean of jump\", \"variance of jump\", \"strike\", \"call_price\"])\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(self.calculate_for_parameters)(lam, m, var, strike)\n",
    "            for lam in range(8)\n",
    "            for m in np.arange(0, 1, 0.013)\n",
    "            for var in np.arange(0, 2, 0.013)\n",
    "            for strike in [self.option_time['Strike'][self.Strike]]\n",
    "        )\n",
    "\n",
    "        for result in results:\n",
    "            self.call_jd.loc[len(self.call_jd)] = result\n",
    "\n",
    "        return self.call_jd\n",
    "\n",
    "    def calculate_for_parameters(self, lam, m, var, strike):\n",
    "\n",
    "        jump = JumpDiffusion(self.ticker_stock['Open'].item(), strike, self.sigma, 0.045, self.time, np.exp(m+var**2 * 0.5) , var, lam)\n",
    "\n",
    "        return [lam, m, var, strike, jump.JD_Call()]\n",
    "\n",
    "    def compare(self):\n",
    "\n",
    "        real_price = self.option_time[self.option_time.Strike == self.option_time['Strike'][self.Strike]]['Last Price'].item()\n",
    "        self.call_jd['diff'] = np.abs(self.call_jd.call_price - real_price)\n",
    "        self.jd = self.call_jd.sort_values(by='diff', ascending=True)\n",
    "\n",
    "        return self.jd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = get_ticker()\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "for ticker in tqdm(ticker_list[:2]):\n",
    "    try:\n",
    "        a = work(ticker)\n",
    "        a.stupid_simu()\n",
    "        results = a.compare()\n",
    "        results['ticker'] = ticker\n",
    "        Y_list.append(results)\n",
    "\n",
    "        daily_change = get_daily_change([ticker])\n",
    "        daily_change = pd.concat([daily_change]*3, ignore_index=True)\n",
    "        daily_change['ticker'] = ticker\n",
    "        X_list.append(daily_change)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with ticker {ticker}: {e}\")\n",
    "\n",
    "X = pd.concat(X_list).sort_values('ticker')\n",
    "Y = pd.concat(Y_list).sort_values('ticker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T03:39:29.928283Z",
     "start_time": "2023-06-05T03:39:22.868179Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read X and Y data from Excel files\n",
    "X = pd.read_excel('X.xlsx')\n",
    "Y = pd.read_excel('Y.xlsx')\n",
    "\n",
    "# Select the columns in X for input\n",
    "X_input = X.iloc[:, :-2].dropna()\n",
    "X_input = X_input.iloc[:, 2:]\n",
    "\n",
    "# Select the corresponding rows in Y for input\n",
    "Y_input = Y.iloc[:, [1, 2, 3]].iloc[X_input.index, :]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_input, Y_input, test_size=0.2)\n",
    "\n",
    "# Normalize the X data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled).float()\n",
    "X_test_tensor = torch.tensor(X_test_scaled).float()\n",
    "y_train_tensor = torch.tensor(Y_train.values).float()\n",
    "y_test_tensor = torch.tensor(Y_test.values).float()\n",
    "\n",
    "# Reshape the tensors for LSTM input\n",
    "X_train_tensor = X_train_tensor.view(X_train_tensor.shape[0], X_train_tensor.shape[1], 1)\n",
    "X_test_tensor = X_test_tensor.view(X_test_tensor.shape[0], X_test_tensor.shape[1], 1)\n",
    "y_train_tensor = y_train_tensor.view(y_train_tensor.shape[0], 3)\n",
    "y_test_tensor = y_test_tensor.view(y_test_tensor.shape[0], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:24<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 50\n",
    "output_size = 3\n",
    "\n",
    "model = LSTM(input_size, hidden_size, 1, output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = loss_function(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6399, 0.3027, 0.5496],\n",
       "        [3.6392, 0.3031, 0.5511],\n",
       "        [3.6401, 0.3028, 0.5494],\n",
       "        [3.6403, 0.3028, 0.5491],\n",
       "        [3.6402, 0.3030, 0.5495],\n",
       "        [3.6414, 0.3025, 0.5472],\n",
       "        [3.6421, 0.3025, 0.5459],\n",
       "        [3.6391, 0.3029, 0.5509],\n",
       "        [3.6408, 0.3028, 0.5483],\n",
       "        [3.6391, 0.3029, 0.5509],\n",
       "        [3.6396, 0.3030, 0.5504],\n",
       "        [3.6400, 0.3028, 0.5496],\n",
       "        [3.6407, 0.3029, 0.5487],\n",
       "        [3.6425, 0.3028, 0.5458],\n",
       "        [3.6412, 0.3028, 0.5477],\n",
       "        [3.6411, 0.3028, 0.5479],\n",
       "        [3.6395, 0.3030, 0.5504],\n",
       "        [3.6409, 0.3028, 0.5481],\n",
       "        [3.6403, 0.3026, 0.5489],\n",
       "        [3.6400, 0.3028, 0.5497],\n",
       "        [3.6396, 0.3029, 0.5501],\n",
       "        [3.6405, 0.3028, 0.5487],\n",
       "        [3.6403, 0.3029, 0.5493],\n",
       "        [3.6388, 0.3032, 0.5517],\n",
       "        [3.6400, 0.3027, 0.5494],\n",
       "        [3.6403, 0.3028, 0.5491],\n",
       "        [3.6430, 0.3027, 0.5448],\n",
       "        [3.6409, 0.3029, 0.5484],\n",
       "        [3.6408, 0.3029, 0.5486],\n",
       "        [3.6380, 0.3031, 0.5527],\n",
       "        [3.6409, 0.3029, 0.5484],\n",
       "        [3.6404, 0.3030, 0.5492],\n",
       "        [3.6394, 0.3030, 0.5506],\n",
       "        [3.6391, 0.3029, 0.5509],\n",
       "        [3.6402, 0.3028, 0.5492],\n",
       "        [3.6404, 0.3029, 0.5491],\n",
       "        [3.6421, 0.3030, 0.5466],\n",
       "        [3.6407, 0.3029, 0.5486],\n",
       "        [3.6405, 0.3027, 0.5488],\n",
       "        [3.6430, 0.3028, 0.5451],\n",
       "        [3.6407, 0.3028, 0.5485],\n",
       "        [3.6402, 0.3026, 0.5489],\n",
       "        [3.6421, 0.3031, 0.5468],\n",
       "        [3.6399, 0.3027, 0.5496],\n",
       "        [3.6391, 0.3031, 0.5513],\n",
       "        [3.6403, 0.3029, 0.5493],\n",
       "        [3.6432, 0.3028, 0.5446],\n",
       "        [3.6398, 0.3026, 0.5496],\n",
       "        [3.6411, 0.3033, 0.5482],\n",
       "        [3.6401, 0.3030, 0.5497],\n",
       "        [3.6401, 0.3030, 0.5497],\n",
       "        [3.6407, 0.3027, 0.5483],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6395, 0.3031, 0.5506],\n",
       "        [3.6403, 0.3029, 0.5492],\n",
       "        [3.6398, 0.3028, 0.5498],\n",
       "        [3.6396, 0.3029, 0.5503],\n",
       "        [3.6402, 0.3030, 0.5495],\n",
       "        [3.6409, 0.3029, 0.5484],\n",
       "        [3.6405, 0.3031, 0.5490],\n",
       "        [3.6389, 0.3026, 0.5508],\n",
       "        [3.6403, 0.3028, 0.5492],\n",
       "        [3.6403, 0.3029, 0.5493],\n",
       "        [3.6395, 0.3028, 0.5502],\n",
       "        [3.6414, 0.3030, 0.5477],\n",
       "        [3.6408, 0.3027, 0.5483],\n",
       "        [3.6402, 0.3028, 0.5493],\n",
       "        [3.6408, 0.3027, 0.5482],\n",
       "        [3.6393, 0.3030, 0.5509],\n",
       "        [3.6393, 0.3030, 0.5509],\n",
       "        [3.6414, 0.3029, 0.5476],\n",
       "        [3.6400, 0.3028, 0.5496],\n",
       "        [3.6386, 0.3027, 0.5515],\n",
       "        [3.6409, 0.3027, 0.5482],\n",
       "        [3.6414, 0.3029, 0.5476],\n",
       "        [3.6410, 0.3029, 0.5483],\n",
       "        [3.6400, 0.3029, 0.5496],\n",
       "        [3.6403, 0.3028, 0.5492],\n",
       "        [3.6405, 0.3027, 0.5487],\n",
       "        [3.6410, 0.3027, 0.5480],\n",
       "        [3.6393, 0.3031, 0.5509],\n",
       "        [3.6403, 0.3028, 0.5491],\n",
       "        [3.6407, 0.3031, 0.5488],\n",
       "        [3.6393, 0.3031, 0.5510],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6400, 0.3029, 0.5497],\n",
       "        [3.6415, 0.3030, 0.5475],\n",
       "        [3.6397, 0.3030, 0.5502],\n",
       "        [3.6406, 0.3029, 0.5489],\n",
       "        [3.6420, 0.3028, 0.5466],\n",
       "        [3.6391, 0.3029, 0.5510],\n",
       "        [3.6405, 0.3028, 0.5488],\n",
       "        [3.6413, 0.3030, 0.5479],\n",
       "        [3.6403, 0.3028, 0.5492],\n",
       "        [3.6405, 0.3027, 0.5486],\n",
       "        [3.6403, 0.3028, 0.5491],\n",
       "        [3.6407, 0.3028, 0.5485],\n",
       "        [3.6408, 0.3031, 0.5487],\n",
       "        [3.6409, 0.3029, 0.5484],\n",
       "        [3.6408, 0.3027, 0.5483],\n",
       "        [3.6399, 0.3030, 0.5499],\n",
       "        [3.6407, 0.3028, 0.5485],\n",
       "        [3.6391, 0.3030, 0.5511],\n",
       "        [3.6401, 0.3028, 0.5494],\n",
       "        [3.6405, 0.3029, 0.5488],\n",
       "        [3.6433, 0.3029, 0.5442],\n",
       "        [3.6393, 0.3028, 0.5506],\n",
       "        [3.6405, 0.3028, 0.5488],\n",
       "        [3.6415, 0.3029, 0.5475],\n",
       "        [3.6398, 0.3028, 0.5497],\n",
       "        [3.6408, 0.3029, 0.5486],\n",
       "        [3.6398, 0.3031, 0.5501],\n",
       "        [3.6405, 0.3028, 0.5488],\n",
       "        [3.6403, 0.3026, 0.5488],\n",
       "        [3.6399, 0.3025, 0.5491],\n",
       "        [3.6404, 0.3030, 0.5492],\n",
       "        [3.6410, 0.3030, 0.5482],\n",
       "        [3.6408, 0.3027, 0.5482],\n",
       "        [3.6404, 0.3028, 0.5490],\n",
       "        [3.6396, 0.3028, 0.5501],\n",
       "        [3.6388, 0.3030, 0.5514],\n",
       "        [3.6402, 0.3028, 0.5492],\n",
       "        [3.6408, 0.3030, 0.5487],\n",
       "        [3.6391, 0.3030, 0.5510],\n",
       "        [3.6405, 0.3028, 0.5488],\n",
       "        [3.6415, 0.3029, 0.5475],\n",
       "        [3.6413, 0.3028, 0.5476],\n",
       "        [3.6387, 0.3030, 0.5516],\n",
       "        [3.6409, 0.3029, 0.5484],\n",
       "        [3.6399, 0.3030, 0.5499],\n",
       "        [3.6421, 0.3025, 0.5459],\n",
       "        [3.6413, 0.3030, 0.5479],\n",
       "        [3.6402, 0.3030, 0.5495],\n",
       "        [3.6430, 0.3028, 0.5451],\n",
       "        [3.6429, 0.3026, 0.5447],\n",
       "        [3.6409, 0.3028, 0.5482],\n",
       "        [3.6412, 0.3029, 0.5478],\n",
       "        [3.6399, 0.3029, 0.5498],\n",
       "        [3.6405, 0.3030, 0.5491],\n",
       "        [3.6433, 0.3029, 0.5442],\n",
       "        [3.6418, 0.3030, 0.5471],\n",
       "        [3.6400, 0.3028, 0.5496],\n",
       "        [3.6390, 0.3029, 0.5512],\n",
       "        [3.6414, 0.3025, 0.5472],\n",
       "        [3.6411, 0.3026, 0.5476],\n",
       "        [3.6403, 0.3031, 0.5494],\n",
       "        [3.6405, 0.3028, 0.5488],\n",
       "        [3.6401, 0.3027, 0.5492],\n",
       "        [3.6414, 0.3028, 0.5475],\n",
       "        [3.6396, 0.3028, 0.5502],\n",
       "        [3.6394, 0.3030, 0.5506],\n",
       "        [3.6399, 0.3029, 0.5498],\n",
       "        [3.6408, 0.3030, 0.5486],\n",
       "        [3.6393, 0.3029, 0.5507],\n",
       "        [3.6395, 0.3030, 0.5506],\n",
       "        [3.6412, 0.3029, 0.5478],\n",
       "        [3.6398, 0.3028, 0.5498],\n",
       "        [3.6408, 0.3028, 0.5483],\n",
       "        [3.6396, 0.3030, 0.5504],\n",
       "        [3.6414, 0.3029, 0.5476],\n",
       "        [3.6403, 0.3029, 0.5493],\n",
       "        [3.6396, 0.3030, 0.5504],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6386, 0.3030, 0.5518],\n",
       "        [3.6366, 0.3031, 0.5545],\n",
       "        [3.6400, 0.3030, 0.5498],\n",
       "        [3.6450, 0.3024, 0.5412],\n",
       "        [3.6395, 0.3031, 0.5506],\n",
       "        [3.6433, 0.3028, 0.5445],\n",
       "        [3.6395, 0.3028, 0.5503],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6399, 0.3027, 0.5494],\n",
       "        [3.6412, 0.3029, 0.5480],\n",
       "        [3.6404, 0.3030, 0.5491],\n",
       "        [3.6414, 0.3025, 0.5472],\n",
       "        [3.6401, 0.3028, 0.5494],\n",
       "        [3.6396, 0.3029, 0.5503],\n",
       "        [3.6402, 0.3029, 0.5494],\n",
       "        [3.6401, 0.3031, 0.5498],\n",
       "        [3.6383, 0.3027, 0.5518],\n",
       "        [3.6402, 0.3028, 0.5492],\n",
       "        [3.6388, 0.3030, 0.5515],\n",
       "        [3.6400, 0.3026, 0.5493],\n",
       "        [3.6406, 0.3028, 0.5487],\n",
       "        [3.6402, 0.3027, 0.5491],\n",
       "        [3.6402, 0.3028, 0.5493],\n",
       "        [3.6384, 0.3028, 0.5517],\n",
       "        [3.6400, 0.3028, 0.5497],\n",
       "        [3.6378, 0.3031, 0.5529],\n",
       "        [3.6397, 0.3030, 0.5503],\n",
       "        [3.6419, 0.3028, 0.5467],\n",
       "        [3.6397, 0.3030, 0.5502],\n",
       "        [3.6412, 0.3027, 0.5476],\n",
       "        [3.6394, 0.3029, 0.5505],\n",
       "        [3.6378, 0.3031, 0.5530],\n",
       "        [3.6402, 0.3030, 0.5495],\n",
       "        [3.6224, 0.3033, 0.5713],\n",
       "        [3.6415, 0.3028, 0.5473],\n",
       "        [3.6396, 0.3029, 0.5503],\n",
       "        [3.6397, 0.3026, 0.5497],\n",
       "        [3.6404, 0.3028, 0.5490],\n",
       "        [3.6412, 0.3030, 0.5479],\n",
       "        [3.6404, 0.3030, 0.5492],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6400, 0.3029, 0.5496],\n",
       "        [3.6403, 0.3028, 0.5491],\n",
       "        [3.6415, 0.3028, 0.5473],\n",
       "        [3.6402, 0.3028, 0.5493],\n",
       "        [3.6402, 0.3028, 0.5492],\n",
       "        [3.6400, 0.3028, 0.5496],\n",
       "        [3.6391, 0.3030, 0.5511],\n",
       "        [3.6406, 0.3028, 0.5487],\n",
       "        [3.6399, 0.3030, 0.5500],\n",
       "        [3.6407, 0.3029, 0.5488],\n",
       "        [3.6401, 0.3028, 0.5495],\n",
       "        [3.6405, 0.3030, 0.5491],\n",
       "        [3.6397, 0.3026, 0.5497],\n",
       "        [3.6399, 0.3029, 0.5498],\n",
       "        [3.6405, 0.3027, 0.5486],\n",
       "        [3.6400, 0.3025, 0.5492],\n",
       "        [3.6400, 0.3029, 0.5498],\n",
       "        [3.6429, 0.3026, 0.5449],\n",
       "        [3.6402, 0.3029, 0.5494],\n",
       "        [3.6391, 0.3029, 0.5510],\n",
       "        [3.6396, 0.3029, 0.5503],\n",
       "        [3.6390, 0.3030, 0.5512],\n",
       "        [3.6408, 0.3028, 0.5483],\n",
       "        [3.6403, 0.3029, 0.5493],\n",
       "        [3.6396, 0.3028, 0.5502],\n",
       "        [3.6407, 0.3029, 0.5486],\n",
       "        [3.6378, 0.3031, 0.5531],\n",
       "        [3.6393, 0.3026, 0.5504],\n",
       "        [3.6413, 0.3028, 0.5476],\n",
       "        [3.6405, 0.3030, 0.5492],\n",
       "        [3.6399, 0.3028, 0.5498],\n",
       "        [3.6398, 0.3026, 0.5496],\n",
       "        [3.6400, 0.3028, 0.5497],\n",
       "        [3.6402, 0.3030, 0.5496],\n",
       "        [3.6399, 0.3029, 0.5497],\n",
       "        [3.6414, 0.3029, 0.5476],\n",
       "        [3.6398, 0.3027, 0.5498],\n",
       "        [3.6391, 0.3031, 0.5513],\n",
       "        [3.6388, 0.3030, 0.5515],\n",
       "        [3.6411, 0.3026, 0.5477],\n",
       "        [3.6410, 0.3028, 0.5478],\n",
       "        [3.6386, 0.3029, 0.5517],\n",
       "        [3.6406, 0.3029, 0.5488],\n",
       "        [3.6407, 0.3026, 0.5484],\n",
       "        [3.6396, 0.3029, 0.5503],\n",
       "        [3.6383, 0.3029, 0.5520],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6419, 0.3029, 0.5469],\n",
       "        [3.6406, 0.3029, 0.5487],\n",
       "        [3.6384, 0.3028, 0.5517],\n",
       "        [3.6405, 0.3030, 0.5492],\n",
       "        [3.6361, 0.3027, 0.5548],\n",
       "        [3.6398, 0.3031, 0.5501],\n",
       "        [3.6405, 0.3030, 0.5491],\n",
       "        [3.6400, 0.3029, 0.5496],\n",
       "        [3.6393, 0.3028, 0.5506],\n",
       "        [3.6387, 0.3027, 0.5511],\n",
       "        [3.6408, 0.3027, 0.5483],\n",
       "        [3.6383, 0.3027, 0.5519],\n",
       "        [3.6391, 0.3029, 0.5509],\n",
       "        [3.6416, 0.3028, 0.5471],\n",
       "        [3.6404, 0.3030, 0.5492],\n",
       "        [3.6407, 0.3027, 0.5483],\n",
       "        [3.6409, 0.3028, 0.5481],\n",
       "        [3.6401, 0.3029, 0.5494],\n",
       "        [3.6425, 0.3028, 0.5458],\n",
       "        [3.6419, 0.3032, 0.5471],\n",
       "        [3.6398, 0.3030, 0.5501],\n",
       "        [3.6411, 0.3029, 0.5480],\n",
       "        [3.6404, 0.3030, 0.5492],\n",
       "        [3.6402, 0.3030, 0.5495],\n",
       "        [3.6401, 0.3029, 0.5496],\n",
       "        [3.6408, 0.3027, 0.5483],\n",
       "        [3.6410, 0.3027, 0.5479],\n",
       "        [3.6407, 0.3025, 0.5482],\n",
       "        [3.6403, 0.3030, 0.5494],\n",
       "        [3.6407, 0.3027, 0.5483],\n",
       "        [3.6400, 0.3028, 0.5497]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "y_pred_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
